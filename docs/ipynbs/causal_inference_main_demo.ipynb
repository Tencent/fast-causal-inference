{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c049a0-885f-41f3-9cba-a42917635da0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Fast Causal inference\n",
    "\n",
    "Fast Causal Inference is Tencent's first open-source causal inference project. It is an OLAP-based high-performance causal inference (statistical model) computing library, which solves the performance bottleneck of existing statistical model libraries (R/Python) under big data, and provides causal inference capabilities for massive data execution in seconds and sub-seconds. At the same time, the threshold for using statistical models is lowered through the SQL language, making it easy to use in production environments. At present, it has supported the causal analysis of WeChat-Search, WeChat-Video-Account and other businesses, greatly improving the work efficiency of data scientists.\n",
    "![Example Image](https://github.com/Tencent/fast-causal-inference/raw/main/docs/images/fast-causal-inference3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a303640-afd2-46c4-ad50-9d9036d6c57e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fb08a2-9751-43d0-84cb-ff82e87b0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6279fa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fast_causal_inference\n",
    "ais = fast_causal_inference.FCIProvider('all_in_sql')\n",
    "df = ais.readStarRocks('test_data_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c829be-2ee3-4d0a-a0cc-e20c0595109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       id            x1            x2           x3           x4           x5 x_long_tail1  x_long_tail2 x_cat1  \\\n",
      "0    0013aaa9-78bd-4421-a485-b06cda2cae7b   -.784221383    2.43174199    .21083256  1.122711457   .627277542   .659296942    .527418381      E   \n",
      "1    0022fc7e-f3fa-4331-b0f1-878f8203540c    .926981889    -.77896849    .06537978  4.728198792  1.303427799   .746145687    .014150644      C   \n",
      "2    002c1878-f994-4b11-8d03-62113f621f1e   -.720615247   -.614475543   .379347097  4.051602288  4.502637162   .111069994    .596422673      C   \n",
      "3    0030d7ee-573d-405e-9609-2c12a364a307   1.342353691    .186825242  1.417049448  2.794842394  4.601687174   .000401808     .72862921      A   \n",
      "4    003598cf-18c6-4aaf-bd34-73cd33ef347c   -.735976592   1.188873059    .24928749   .095214567  3.735083439   .006746523  19.165992615      D   \n",
      "..                                    ...           ...           ...          ...          ...          ...          ...           ...    ...   \n",
      "195  0a1590d2-b79c-4f06-8fe9-ffc9067b1b68   1.584878452    .982470359   .743837206   .149494902  2.891765971   .048990965   3.019839429      E   \n",
      "196  0a1fae31-1b50-477e-a0ba-ca2253ffe073    .160928168  -2.261460961  1.093010627  2.853217152   .019093484   .079010018   1.672477288      E   \n",
      "197  0a364040-b308-4911-86c1-60cb015ec98d    .544354878  -1.133837502   .191578482  4.366799571  3.525414047    .39167646    .045446269      B   \n",
      "198  0a440fe3-6250-432b-b549-e810d1809218  -1.102243024   2.365125051   .131198257  2.174947966  1.740720127  1.223735167    .460586603      C   \n",
      "199  0a458048-0eae-4c87-a560-c68ce5f2218b    .720033759    .505768683   .608837005  5.196566816  2.645033464   .347623605    .090006766      E   \n",
      "\n",
      "    treatment t_ob             y          y_ob numerator_pre     numerator denominator_pre denominator      weight        day_  \n",
      "0           0    1   4.796482619  -1.217059613   6.030571212   4.796482619               1          -1  .816256466  2023-11-03  \n",
      "1           0    1  -3.840697601   2.084558089  -2.721380925  -3.840697601               5           5   .11394116  2023-11-03  \n",
      "2           0    0   1.116095599    .966082316   2.555738984   1.116095599               7           6   .14735276  2023-11-03  \n",
      "3           1    1  18.482986178    .892991166   6.522697065  18.482986178              13          19  .372828283  2023-11-03  \n",
      "4           1    0  20.905855564   1.264433543  12.749023262  20.905855564               6           9  .497844668  2023-11-03  \n",
      "..        ...  ...           ...           ...           ...           ...             ...         ...         ...         ...  \n",
      "195         1    1  22.001118107   -.798909346  11.750711362  22.001118107               7          17  .135662675  2023-11-03  \n",
      "196         0    1  -3.327238022   2.071893605   1.892784192  -3.327238022              -1           0   .27523065  2023-11-03  \n",
      "197         0    0   -1.70851063    .233502085    .738541007   -1.70851063              10           9  .166461763  2023-11-03  \n",
      "198         1    0  22.578294258    .258935982    5.63851678  22.578294258               3           3  .950975032  2023-11-03  \n",
      "199         1    0  14.227877843   -.949859807   4.888458241  14.227877843               8           8  .259669473  2023-11-03  \n",
      "\n",
      "[200 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f77e99-6844-42df-ab0c-c89fa83d6e0c",
   "metadata": {},
   "source": [
    "# know your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2dc3b-7ed2-431d-9878-22968ad72149",
   "metadata": {},
   "source": [
    "### test data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aad3a91-3874-4025-a1d0-8ca45586f6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         x1        x2        x3        x4        x5  x_long_tail1  x_long_tail2 x_cat1  treatment  t_ob          y      y_ob  numerator_pre  \\\n",
      "0  1.764052 -0.404234  0.691733  1.747157  1.567715      0.186325      0.066557      E          1     1  14.341114  0.672431       3.511439   \n",
      "1  0.400157 -1.666462  4.310034  1.393977  4.755300      0.172270      0.059758      C          0     1  12.022749  2.776448       9.612530   \n",
      "2  0.978738  3.467200  1.162678  1.188839  1.739792      0.004183      0.002593      E          1     1  44.461318 -1.288468      10.548653   \n",
      "3  2.240893  0.381298  1.951522  0.235632  0.480281      0.747511      0.114508      E          1     1  17.249192  3.037100       8.715654   \n",
      "4  1.867558 -0.355621  0.703186  1.544799  3.959299      0.168121      1.927359      D          1     1  20.020350  1.589388       7.982669   \n",
      "\n",
      "   numerator  denominator_pre  denominator    weight  \n",
      "0  14.341114         6.554783    11.801924  0.129557  \n",
      "1  12.022749        10.953917     7.550823  0.558684  \n",
      "2  44.461318         4.174754     7.436273  0.382343  \n",
      "3  17.249192         4.696189    12.268354  0.025037  \n",
      "4  20.020350        10.829893    18.515595  0.616490  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n = 10000\n",
    "\n",
    "################################## generate covariables #############################################\n",
    "# Generate x1-x5, they come from different distributions and have different variances\n",
    "x1 = np.random.normal(0, 1, n) # Normal distribution, variance is 1\n",
    "x2 = np.random.normal(0, 2, n)  # Normal distribution, variance is 4\n",
    "x3 = np.random.exponential(1, n)  # Exponential distribution, variance is 1\n",
    "x4 = np.random.exponential(2, n)  # Exponential distribution, variance is 4\n",
    "x5 = np.random.uniform(0, 5, n)  # Uniform distribution, variance is approximately 1.33\n",
    "weight = np.random.uniform(0, 1, n) # use for sample reweighing\n",
    "\n",
    "# Generate x6-x7, they are long-tail distributed data\n",
    "x_long_tail1 = np.random.pareto(3, n)  # Pareto distribution\n",
    "x_long_tail2 = np.random.pareto(2, n)  # Pareto distribution\n",
    "\n",
    "# Generate x_cat1, it is a discrete variable of string type\n",
    "x_cat1 = np.random.choice(['A', 'B', 'C', 'D', 'E'], n)\n",
    "###################################################################################################### \n",
    "\n",
    "\n",
    "\n",
    "################################## generate exprimental data ######################################### \n",
    "# Generate treatment, it is a binary random variable\n",
    "treatment = np.random.choice([0, 1], n)\n",
    "\n",
    "\n",
    "# Generate y \n",
    "# y is highly correlated with x1, x2, x3,x_long_tail2 and treatment\n",
    "# and there is heterogeneity on x_cat1 and x1,x2,x4\n",
    "y_pre = x1 + 2*x2 + 3*x3 + 3*np.log(x_long_tail2+1) + np.random.normal(0, 1, n)\n",
    "y = y_pre + 5*treatment + treatment*(x1+x2**2+np.log(x4+1)+(x_long_tail1>1))*2 + np.random.normal(0, 2, n)\n",
    "y[x_cat1 == 'A'] += 3\n",
    "y[x_cat1 == 'B'] -= 2\n",
    "\n",
    "# Generate ratio metric: numerator/denominator, for example click/show\n",
    "numerator_pre = y_pre \n",
    "numerator = y \n",
    "denominator_pre = x1 + 2*x5 + 3*np.log(x_long_tail1+1) + np.random.normal(0, 1, n)\n",
    "denominator = denominator_pre + 2*treatment + treatment*(x1)*2 + np.random.normal(0, 2, n)\n",
    "###################################################################################################### \n",
    "\n",
    "\n",
    "################################## generate observational data ####################################### \n",
    "# use x1,x2,x3 before\n",
    "# Generate linear combination for t_ob\n",
    "linear_combination_t = 1*x1 + 0.2*x2 + 0.5 * x3 + 0.1 * np.random.normal(0, 1, n)\n",
    "\n",
    "# Convert the linear combination into a probability using the logistic function\n",
    "prob_t = 1 / (1 + np.exp(-linear_combination_t))\n",
    "\n",
    "# Generate binary variable t based on the probability\n",
    "t_ob = np.random.binomial(1, prob_t)\n",
    "\n",
    "# Generate linear combination for y\n",
    "linear_combination_y = 0.5 * x1 - 0.25 * x2 + 0.1 * x3 + 0.5 * t_ob + 0.1 * np.random.normal(0, 1, n)\n",
    "\n",
    "# Generate target variable y\n",
    "y_ob = linear_combination_y + np.random.normal(0, 1, n)\n",
    "###################################################################################################### \n",
    "\n",
    "\n",
    "\n",
    "# get DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x1': x1,\n",
    "    'x2': x2,\n",
    "    'x3': x3,\n",
    "    'x4': x4,\n",
    "    'x5': x5,\n",
    "    'x_long_tail1': x_long_tail1,\n",
    "    'x_long_tail2': x_long_tail2,\n",
    "    'x_cat1': x_cat1,\n",
    "    'treatment': treatment,\n",
    "    't_ob':t_ob,\n",
    "    'y': y,\n",
    "    'y_ob': y_ob,\n",
    "    'numerator_pre':numerator_pre,\n",
    "    'numerator':numerator,\n",
    "    'denominator_pre':denominator_pre,\n",
    "    'denominator':denominator,\n",
    "    'weight' : weight\n",
    "})\n",
    "\n",
    "\n",
    "# show data \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f3e29-ad4e-4318-a054-cb1c8fc3e716",
   "metadata": {},
   "source": [
    "Covariables\n",
    "\n",
    "- `x1`: This variable follows a normal distribution with a mean of 0 and a variance of 1.\n",
    "- `x2`: This variable also follows a normal distribution, but with a mean of 0 and a larger variance of 4.\n",
    "- `x3`: This variable is generated from an exponential distribution with a rate parameter of 1, resulting in a variance of 1.\n",
    "- `x4`: Similar to `x3`, this variable is generated from an exponential distribution, but with a larger rate parameter of 2, resulting in a variance of 4.\n",
    "- `x5`: This variable is generated from a uniform distribution between 0 and 5, with an approximate variance of 1.33.\n",
    "- `weight`: This variable is generated from a uniform distribution between 0 and 1 and can be used for sample reweighing.\n",
    "- `x_long_tail1`: This variable follows a Pareto distribution with a shape parameter of 3, representing a long-tailed distribution.\n",
    "- `x_long_tail2`: Similarly, this variable follows a Pareto distribution with a shape parameter of 2.\n",
    "- `x_cat1`: This variable is a discrete variable of string type, randomly chosen from the categories 'A', 'B', 'C', 'D', and 'E'.\n",
    "\n",
    "Experimental data\n",
    "- `treatment`: This variable represents the treatment assignment and is a binary random variable.\n",
    "\n",
    "- `y`: The target variable `y` is highly correlated with `x1`, `x2`, `x3`, `x_long_tail2`, and `treatment`. There is also heterogeneity in the relationship with `x_cat1` and `x1`, `x2`, `x4`. It is generated by adding `y_pre` with treatment effects, interaction terms, and random noise.\n",
    "\n",
    "- `numerator_pre`: This variable is a precursor to the numerator of a ratio metric and is highly correlated with `y_pre`.\n",
    "\n",
    "- `numerator`: The numerator of the ratio metric is derived from `numerator_pre` and is highly correlated with `y`.\n",
    "\n",
    "- `denominator_pre`: This variable is a precursor to the denominator of a ratio metric and is generated based on `x1`, `x5`, and `x_long_tail1`.\n",
    "\n",
    "- `denominator`: The denominator of the ratio metric is derived from `denominator_pre` and is influenced by treatment effects, interaction terms with `x1`, and random noise.\n",
    "\n",
    "Observational data \n",
    "- `t_ob`: This binary variable is generated based on a linear combination of `x1`, `x2`, and `x3`. The linear combination is converted into a probability using the logistic function, and `t_ob` is generated by sampling from a binomial distribution with the probability.\n",
    "\n",
    "- `y_ob`: The target variable `y_ob` is generated based on a linear combination of `x1`, `x2`, `x3`, and `t_ob`, along with random noise. It represents the outcome variable in the observational data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5bda7-d6b8-44cb-a3e8-27cadcc3c43c",
   "metadata": {},
   "source": [
    "### data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20e8a39-69d2-4a5c-8c6f-b66e075d5fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 string\n",
       "x1                  float\n",
       "x2                  float\n",
       "x3                  float\n",
       "x4                  float\n",
       "x5                  float\n",
       "x_long_tail1        float\n",
       "x_long_tail2        float\n",
       "x_cat1             string\n",
       "treatment          bigint\n",
       "t_ob               bigint\n",
       "y                   float\n",
       "y_ob                float\n",
       "numerator_pre       float\n",
       "numerator           float\n",
       "denominator_pre    bigint\n",
       "denominator        bigint\n",
       "weight              float\n",
       "day_                 date\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ais.readStarRocks('test_data_small')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e0a121-a978-45b8-bd0a-dca42ee1cb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                     id           x1          x2         x3           x4           x5 x_long_tail1 x_long_tail2 x_cat1 treatment  \\\n",
       "0  0013aaa9-78bd-4421-a485-b06cda2cae7b  -.784221383  2.43174199  .21083256  1.122711457   .627277542   .659296942   .527418381      E         0   \n",
       "1  0022fc7e-f3fa-4331-b0f1-878f8203540c   .926981889  -.77896849  .06537978  4.728198792  1.303427799   .746145687   .014150644      C         0   \n",
       "\n",
       "  t_ob             y          y_ob numerator_pre     numerator denominator_pre denominator      weight        day_  \n",
       "0    1   4.796482619  -1.217059613   6.030571212   4.796482619               1          -1  .816256466  2023-11-03  \n",
       "1    1  -3.840697601   2.084558089  -2.721380925  -3.840697601               5           5   .11394116  2023-11-03  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66739f01-aea3-4a63-9778-f8017c4785f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring column `id`, whose type `string` is not numeric.\n",
      "Ignoring column `x_cat1`, whose type `string` is not numeric.\n",
      "Ignoring column `day_`, whose type `date` is not numeric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>quantile_0.25</th>\n",
       "      <th>quantile_0.50</th>\n",
       "      <th>quantile_0.75</th>\n",
       "      <th>quantile_0.90</th>\n",
       "      <th>quantile_0.99</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>-0.018434</td>\n",
       "      <td>0.987606</td>\n",
       "      <td>-3.740101</td>\n",
       "      <td>-0.689692</td>\n",
       "      <td>-0.026596</td>\n",
       "      <td>0.646518</td>\n",
       "      <td>1.254951</td>\n",
       "      <td>2.310680</td>\n",
       "      <td>3.801660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.021976</td>\n",
       "      <td>1.986209</td>\n",
       "      <td>-8.893264</td>\n",
       "      <td>-1.290912</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>1.352757</td>\n",
       "      <td>2.559644</td>\n",
       "      <td>4.743784</td>\n",
       "      <td>7.196620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.993292</td>\n",
       "      <td>0.987922</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.289424</td>\n",
       "      <td>0.693160</td>\n",
       "      <td>1.378255</td>\n",
       "      <td>2.271443</td>\n",
       "      <td>4.440148</td>\n",
       "      <td>9.417911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.985747</td>\n",
       "      <td>1.971388</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.564462</td>\n",
       "      <td>1.392178</td>\n",
       "      <td>2.767125</td>\n",
       "      <td>4.609450</td>\n",
       "      <td>9.126206</td>\n",
       "      <td>19.830242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2.495959</td>\n",
       "      <td>1.437626</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>1.244424</td>\n",
       "      <td>2.508380</td>\n",
       "      <td>3.736480</td>\n",
       "      <td>4.480969</td>\n",
       "      <td>4.946681</td>\n",
       "      <td>4.999841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_long_tail1</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.515560</td>\n",
       "      <td>0.883486</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.099894</td>\n",
       "      <td>0.259858</td>\n",
       "      <td>0.591583</td>\n",
       "      <td>1.191490</td>\n",
       "      <td>3.917824</td>\n",
       "      <td>28.065146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_long_tail2</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.008366</td>\n",
       "      <td>3.048051</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.152396</td>\n",
       "      <td>0.410504</td>\n",
       "      <td>1.003290</td>\n",
       "      <td>2.135360</td>\n",
       "      <td>9.445344</td>\n",
       "      <td>156.238517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0.499937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_ob</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.599100</td>\n",
       "      <td>0.490105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>12.359893</td>\n",
       "      <td>12.765055</td>\n",
       "      <td>-13.296274</td>\n",
       "      <td>4.145801</td>\n",
       "      <td>10.012142</td>\n",
       "      <td>17.297520</td>\n",
       "      <td>26.935087</td>\n",
       "      <td>58.579937</td>\n",
       "      <td>148.732617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_ob</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.377476</td>\n",
       "      <td>1.293226</td>\n",
       "      <td>-5.128077</td>\n",
       "      <td>-0.482804</td>\n",
       "      <td>0.375775</td>\n",
       "      <td>1.254632</td>\n",
       "      <td>2.033421</td>\n",
       "      <td>3.421490</td>\n",
       "      <td>5.114904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numerator_pre</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>4.499999</td>\n",
       "      <td>5.309163</td>\n",
       "      <td>-14.775517</td>\n",
       "      <td>0.891327</td>\n",
       "      <td>4.220972</td>\n",
       "      <td>7.840807</td>\n",
       "      <td>11.308697</td>\n",
       "      <td>18.509924</td>\n",
       "      <td>36.174521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numerator</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>12.359893</td>\n",
       "      <td>12.765055</td>\n",
       "      <td>-13.296274</td>\n",
       "      <td>4.145801</td>\n",
       "      <td>10.012142</td>\n",
       "      <td>17.297520</td>\n",
       "      <td>26.935087</td>\n",
       "      <td>58.579937</td>\n",
       "      <td>148.732617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denominator_pre</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.521000</td>\n",
       "      <td>3.324825</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denominator</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>6.498400</td>\n",
       "      <td>4.452435</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>17.333334</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.500305</td>\n",
       "      <td>0.287952</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.249210</td>\n",
       "      <td>0.499449</td>\n",
       "      <td>0.749299</td>\n",
       "      <td>0.896570</td>\n",
       "      <td>0.989570</td>\n",
       "      <td>0.999931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count        avg        std        min  quantile_0.25  quantile_0.50  quantile_0.75  quantile_0.90  quantile_0.99         max\n",
       "x1               10000.0  -0.018434   0.987606  -3.740101      -0.689692      -0.026596       0.646518       1.254951       2.310680    3.801660\n",
       "x2               10000.0   0.021976   1.986209  -8.893264      -1.290912       0.009334       1.352757       2.559644       4.743784    7.196620\n",
       "x3               10000.0   0.993292   0.987922   0.000439       0.289424       0.693160       1.378255       2.271443       4.440148    9.417911\n",
       "x4               10000.0   1.985747   1.971388   0.000400       0.564462       1.392178       2.767125       4.609450       9.126206   19.830242\n",
       "x5               10000.0   2.495959   1.437626   0.000984       1.244424       2.508380       3.736480       4.480969       4.946681    4.999841\n",
       "x_long_tail1     10000.0   0.515560   0.883486   0.000021       0.099894       0.259858       0.591583       1.191490       3.917824   28.065146\n",
       "x_long_tail2     10000.0   1.008366   3.048051   0.000002       0.152396       0.410504       1.003290       2.135360       9.445344  156.238517\n",
       "treatment        10000.0   0.509400   0.499937   0.000000       0.000000       1.000000       1.000000       1.000000       1.000000    1.000000\n",
       "t_ob             10000.0   0.599100   0.490105   0.000000       0.000000       1.000000       1.000000       1.000000       1.000000    1.000000\n",
       "y                10000.0  12.359893  12.765055 -13.296274       4.145801      10.012142      17.297520      26.935087      58.579937  148.732617\n",
       "y_ob             10000.0   0.377476   1.293226  -5.128077      -0.482804       0.375775       1.254632       2.033421       3.421490    5.114904\n",
       "numerator_pre    10000.0   4.499999   5.309163 -14.775517       0.891327       4.220972       7.840807      11.308697      18.509924   36.174521\n",
       "numerator        10000.0  12.359893  12.765055 -13.296274       4.145801      10.012142      17.297520      26.935087      58.579937  148.732617\n",
       "denominator_pre  10000.0   5.521000   3.324825  -3.000000       3.000000       5.000000       8.000000      10.000000      13.000000   18.000000\n",
       "denominator      10000.0   6.498400   4.452435 -10.000000       3.000000       6.000000       9.000000      12.000000      17.333334   24.000000\n",
       "weight           10000.0   0.500305   0.287952   0.000091       0.249210       0.499449       0.749299       0.896570       0.989570    0.999931"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data descirption\n",
    "df.describe('*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01fd229-ffed-4b93-a6a9-0c17ac54835e",
   "metadata": {},
   "source": [
    "# AB experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058363b-5e71-48dc-ab22-cd51d85e8c8c",
   "metadata": {},
   "source": [
    "### ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af779c9a-822b-44f2-be2f-bf242eb5e792",
   "metadata": {},
   "source": [
    "- In A/B test analysis, **t-test** is widely used to test whether the average of treatment variant is statistically significantly different from the average of the control variant. However, the mean and variance formula only applied to i.i.d (independent and identically distributed) random variables, and in real business cases our metrics are more complex. Mostly, business metrics are defined as ratios, for example, Clickthrough rate or CTR which is defined as Clicks/Views. Here, we will *utilize the delta method to approximate the variance of the metrics ratio*.\n",
    "$$t=\\frac{\\bar X_B-\\bar X_A}{\\sqrt{Var(\\bar X_A)+Var(\\bar X_B)}}$$\n",
    "\n",
    "$$Var(\\bar X)=\\frac{1}{n}\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2 $$\n",
    "- **Delta method** extends the normal approximations of the central limit theorem. Delta method approximates asymptotically normal random variables by applying the Taylor series on the function of random variables. We can estimate the variance of x/y as follows:\n",
    "$$v=(1/y,-x/y^2)|_{x=\\bar x, y=\\bar y )}$$\n",
    "\n",
    "$$\\mathop{{M}}\\nolimits_{{2 \\times 2}}=\\frac{1}{n}{ \\left[ {\\begin{array}{*{20}{c}}\n",
    "{var(x)}&{cov(x,y)}\\\\\n",
    "{cov(x,y)}&{var(y)}\\\\\n",
    "\\end{array}} \\right] }={ \\left[ {\\mathop{{m}}\\nolimits_{{ij}}} \\right] }  $$\n",
    "\n",
    "$$var(x/y)=v*\\mathop{{M}}\\nolimits_{{2 \\times 2}}*v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99a160-627e-4f2e-959d-dc1d5f2931c8",
   "metadata": {},
   "source": [
    "- <font size=\"4\">case1: use deltamethod to do t-test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d066d0-9dd5-4bc3-a62d-21d504e941e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      mean0     mean1  estimate    stderr t-statistic   p-value     lower     upper\n",
       "0  0.858954  2.636155  1.777200  0.035934   49.457789  0.000000  1.706763  1.847638"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ttest_2samp('avg(numerator)/avg(denominator)','treatment','two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e1391-90e8-4272-9fd7-efb0e10e68e8",
   "metadata": {},
   "source": [
    "- <font size=\"4\">case2: use deltamethod to do t-test and **CUPED** to reduce variance</font>  \n",
    "\n",
    "\n",
    "    - We offer a feature in ttest_2samp that utilizes [CUPED (Controlled-experiment Using Pre-Experiment Data)](https://ai.stanford.edu/~ronnyk/2013-02CUPEDImprovingSensitivityOfControlledExperiments.pdf) This technique adjusts metrics using pre-experiment data from both control and treatment groups, aiming to decrease metric variability.\n",
    "    - To further reduce variance, you can combine multiple metrics using the '+' operator.\n",
    "    - Additionally, ttest_2samp allows you to perform a t-test grouped by any dimensions of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196c6c8c-4d0c-4ee7-8fa3-88e51272b596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      mean0     mean1  estimate    stderr t-statistic   p-value     lower     upper\n",
       "0  0.858569  2.636480  1.777910  0.027864   63.806431  0.000000  1.723291  1.832529"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ttest_2samp('avg(numerator)/avg(denominator)','treatment','two-sided',X='avg(numerator_pre)/avg(denominator_pre)+avg(x1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee6c590-4286-43ce-ae35-d46ba1945498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  x_cat1     mean0     mean1  estimate    stderr t-statistic   p-value     lower     upper\n",
       "0      D  0.846166  2.606121  1.759955  0.063559   27.690176  0.000000  1.635306  1.884603\n",
       "1      B  0.440779  2.334651  1.893872  0.058136   32.576650  0.000000  1.779857  2.007887\n",
       "2      C  0.841622  2.723429  1.881807  0.070967   26.516686  0.000000  1.742629  2.020985\n",
       "3      E  0.768860  2.584578  1.815718  0.060334   30.094202  0.000000  1.697393  1.934043\n",
       "4      A  1.339015  2.935081  1.596066  0.065873   24.229521  0.000000  1.466882  1.725250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('x_cat1').ttest_2samp('avg(numerator)/avg(denominator)','treatment','two-sided',X='avg(numerator_pre)/avg(denominator_pre)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372f881-39e5-4b31-a133-a4699f7081e6",
   "metadata": {},
   "source": [
    "- <font size=\"4\">case3: ttest_1samp </font>  \n",
    "    - ttest_1samp can be used in pair test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca383d6-476a-4058-ae3e-1e18b599374a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   estimate    stderr t-statistic   p-value     lower     upper\n",
       "0  1.901990  0.020915   90.939969  0.000000  1.860993  1.942987"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ttest_1samp('avg(numerator)/avg(denominator)','two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c219-bb55-42c5-95c3-8ad0e2477236",
   "metadata": {},
   "source": [
    "### MWU test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc609f8a-24e5-4106-a5cd-81cd6440467e",
   "metadata": {},
   "source": [
    "The [Mann–Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2560909-59f1-4244-a282-da8703b86bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2379991, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mann_whitney_utest('numerator', 'treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab336479-8dca-4b22-b1fa-66a4d3c15130",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf25177-a5ed-4f54-8d0e-2ad1be3b02a3",
   "metadata": {},
   "source": [
    "**SRM (Sample ratio mismatch)** is an experimental flaw where the expected traffic allocation doesn’t fit with the observed visitor number for each testing variation. We do this using the chi-squared test of independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557bef2a-d380-4a5f-b7fc-c4523224b55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  groupname         f_obs     ratio     chisquare   p-value\n",
       "0         0  23058.627723  1.000000  48571.698643  0.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.srm('numerator', 'treatment', [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a57cf-c74d-48b2-9390-ffbeaaa634e4",
   "metadata": {},
   "source": [
    "# Regression-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373bd1f-1926-439a-b57b-ff9ccea2aaab",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36b6e3-fd2d-47b3-89cb-5b46c18dfac9",
   "metadata": {},
   "source": [
    "In statistics, [linear regression](https://en.wikipedia.org/wiki/Linear_regression) is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression. Written in matrix notation as:\n",
    "$$y=X\\beta+\\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f603ba67-8f00-43a7-8296-c058ab8405eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ceb56-9228-4044-8e24-6cb02387951b",
   "metadata": {},
   "source": [
    "In statistics, [ordinary least squares (OLS)](https://en.wikipedia.org/wiki/Ordinary_least_squares) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\n",
    "$$S(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left| y_i - \\sum_{j=1}^p X_{ij}\\beta_j\\right|^2 = \\left\\|\\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\right\\|^2.$$\n",
    "We can leverage **matrix multiplication** to compute Ordinary Least Squares (OLS), which is highly efficient in the OLAP engine we are using.\n",
    "$$\\hat{\\boldsymbol{\\beta}} = \\left( \\mathbf{X}^{\\operatorname{T}} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\operatorname{T}} \\mathbf y.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59f50a78-6298-4bc3-8732-562392a402f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "  lm( formula = y ~ x1 + x2 + x3 + weight )\n",
      "\n",
      "Coefficients:\n",
      ".               Estimate    Std. Error  t value     Pr(>|t|)    \n",
      "(Intercept)     9.051493    0.257191    35.193599   0.000000    \n",
      "x1              1.965833    0.116702    16.844859   0.000000    \n",
      "x2              2.168934    0.058077    37.345693   0.000000    \n",
      "x3              3.028528    0.116766    25.936792   0.000000    \n",
      "weight          0.577169    0.400333    1.441721    0.149413    \n",
      "\n",
      "Residual standard error: 11.524357 on 9995 degrees of freedom\n",
      "Multiple R-squared: 0.185269, Adjusted R-squared: 0.184943\n",
      "F-statistic: 568.213021 on 4 and 9995 DF,  p-value: 0.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ols\n",
    "df.ols('y~x1+x2+x3+weight', use_bias=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385d8412-d83d-457f-99e5-c3d693da4164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "  lm( formula = y ~ x1 + x2 + t_ob )\n",
      "\n",
      "Coefficients:\n",
      ".               Estimate    Std. Error  t value     Pr(>|t|)    \n",
      "(Intercept)     11.511121   0.198929    57.865567   0.000000    \n",
      "x1              1.719414    0.129866    13.239884   0.000000    \n",
      "x2              2.052233    0.060800    33.753934   0.000000    \n",
      "t_ob            1.394370    0.265055    5.260684    0.000000    \n",
      "\n",
      "Residual standard error: 11.890735 on 9996 degrees of freedom\n",
      "Multiple R-squared: 0.132555, Adjusted R-squared: 0.132295\n",
      "F-statistic: 509.167914 on 3 and 9996 DF,  p-value: 0.000000\n",
      "\n",
      "                                       id            x1            x2           x3           x4           x5 x_long_tail1 x_long_tail2 x_cat1  \\\n",
      "0    0000e529-e751-4f13-a04c-642fe2de453e   1.147215774   2.415858286  1.500098323  0.209206196  3.908700241  0.147350970  0.683350882      C   \n",
      "1    0013aaa9-78bd-4421-a485-b06cda2cae7b  -0.784221383   2.431741990  0.210832560  1.122711457  0.627277542  0.659296942  0.527418381      E   \n",
      "2    00201035-1189-444d-ae7b-4ecaecdbb486  -0.625245923  -1.173472479  0.623605201  0.250391030  1.122240526  2.788186338  0.161645570      B   \n",
      "3    00234de2-234a-41e8-923a-50e533b0436a  -0.301094787   2.806224008  0.588475536  0.143775637  0.487859027  0.013042122  1.159265315      D   \n",
      "4    002c1878-f994-4b11-8d03-62113f621f1e  -0.720615247  -0.614475543  0.379347097  4.051602288  4.502637162  0.111069994  0.596422673      C   \n",
      "..                                    ...           ...           ...          ...          ...          ...          ...          ...    ...   \n",
      "195  09baf1cf-a266-4517-b845-c30cc0e0d918  -1.510664175   1.783909686  0.232290022  4.941048610  4.432977255  0.503201697  0.246060565      E   \n",
      "196  09becf67-111f-48c3-b902-9b6b954d8177   0.047335821   1.060050076  1.256460383  1.615741476  4.965953183  0.209269391  4.142960096      A   \n",
      "197  09c0b1f7-0fff-4238-9dc7-feb025c81aea   0.330431667   2.434181244  0.170955747  0.127313887  1.812962909  2.464952235  0.303228599      E   \n",
      "198  09d466a8-3ac4-4701-9859-c6ce2e8acb26   0.371955552  -2.954149544  0.758398449  1.423790394  0.606186383  0.268195622  0.285095039      D   \n",
      "199  09eea711-2b6c-48f2-8c52-93ad546b25ab  -0.964612014  -0.203979004  2.021939409  1.820298524  1.040429059  0.392945239  0.651503109      C   \n",
      "\n",
      "    treatment t_ob             y          y_ob numerator_pre     numerator denominator_pre denominator       weight        day_        effect  \n",
      "0           1    1  30.982204856   1.175183768  11.923674304  30.982204856              10          18  0.700279607  2023-11-03  20.533256932  \n",
      "1           0    1   4.796482619  -1.217059613   6.030571212   4.796482619               1          -1  0.816256466  2023-11-03  15.447200851  \n",
      "2           0    0  -2.573179451  -1.524416274   0.112034609  -2.573179451               4           4  0.930103209  2023-11-03   8.897362057  \n",
      "3           0    1   9.606132756   0.669559972   8.156614129   9.606132756               0           1  0.818077900  2023-11-03  17.572993904  \n",
      "4           0    0   1.116095599   0.966082316   2.555738984   1.116095599               7           6  0.147352760  2023-11-03   9.539988633  \n",
      "..        ...  ...           ...           ...           ...           ...             ...         ...          ...         ...           ...  \n",
      "195         0    0   7.739471181   0.091992928   4.292118657   7.739471181               7           9  0.107498534  2023-11-03  12.898561875  \n",
      "196         1    0  23.524219865   0.857271748  11.605658379  23.524219865              11          14  0.358889373  2023-11-03  15.519952132  \n",
      "197         1    1  29.187143649   0.174878732   6.596827540  29.187143649               7          11  0.416124879  2023-11-03  17.313153274  \n",
      "198         1    1  27.560620564   2.326827544  -0.817148419  27.560620564               3          10  0.140375010  2023-11-03   7.145550671  \n",
      "199         1    1  13.235162279   0.960947739   7.935362140  13.235162279               2           2  0.330809935  2023-11-03  12.253273965  \n",
      "\n",
      "[200 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "import fast_causal_inference.dataframe.regression as Regression\n",
    "table = 'test_data_small'\n",
    "df = fast_causal_inference.readStarRocks(table)\n",
    "model = Regression.Ols(True)\n",
    "model.fit('y~x1+x2+t_ob', df)\n",
    "model.summary()\n",
    "effect_df = model.effect('x1+x2+x3', df)\n",
    "effect_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4d3ac-bc9b-4f2f-b610-2a96f46fbaa6",
   "metadata": {},
   "source": [
    "#### WLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52063db6-fce9-4794-bfd0-2b7a4986ea45",
   "metadata": {},
   "source": [
    "[Weighted least squares (WLS)](https://en.wikipedia.org/wiki/Weighted_least_squares), also known as weighted linear regression,[1][2] is a generalization of ordinary least squares and linear regression in which knowledge of the unequal variance of observations (heteroscedasticity) is incorporated into the regression. WLS is also a specialization of generalized least squares, when all the off-diagonal entries of the covariance matrix of the errors, are null.\n",
    "$$  \\underset{\\boldsymbol\\beta}{\\operatorname{arg\\ min}}\\, \\sum_{i=1}^{n} w_i \\left|y_i - \\sum_{j=1}^{m} X_{ij}\\beta_j\\right|^2 =\n",
    "  \\underset{\\boldsymbol\\beta}{\\operatorname{arg\\ min}}\\, \\left\\|W^\\frac{1}{2}\\left(\\mathbf{y} - X\\boldsymbol\\beta\\right)\\right\\|^2.$$\n",
    "We can leverage **matrix multiplication** to compute Weighted least squares (WLS), which is highly efficient in the OLAP engine we are using.\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (X^\\textsf{T} W X)^{-1} X^\\textsf{T} W \\mathbf{y}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f97250e-f0fc-4eb2-ac56-b24e2baff801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "  lm( formula = y ~ x1 + x2 + x3 )\n",
      "\n",
      "Coefficients:\n",
      ".               Estimate    Std. Error  t value     Pr(>|t|)    \n",
      "(Intercept)     9.337661    0.163560    57.090083   0.000000    \n",
      "x1              1.966956    0.116706    16.853952   0.000000    \n",
      "x2              2.167861    0.058076    37.328274   0.000000    \n",
      "x3              3.031182    0.116757    25.961356   0.000000    \n",
      "\n",
      "Residual standard error: 11.524979 on 9996 degrees of freedom\n",
      "Multiple R-squared: 0.185100, Adjusted R-squared: 0.184855\n",
      "F-statistic: 756.842845 on 3 and 9996 DF,  p-value: 0.000000\n",
      "\n",
      "                                       id            x1            x2           x3           x4           x5 x_long_tail1 x_long_tail2 x_cat1  \\\n",
      "0    0017e145-28ac-48ba-b6cd-5ee581da8be9   1.437594358  -0.957589743  0.368883827  0.273081111  3.681733397  0.290041805  0.094332574      D   \n",
      "1    0021469f-676c-4cc5-a75a-27fed536602e  -0.198398897   2.300478296  0.284235675  7.704984329  3.582267588  0.348869983  0.098124044      E   \n",
      "2    0022fc7e-f3fa-4331-b0f1-878f8203540c   0.926981889  -0.778968490  0.065379780  4.728198792  1.303427799  0.746145687  0.014150644      C   \n",
      "3    0028119c-1dc7-478d-bd03-5954cb0a3385  -0.808647839  -0.074160497  0.166398957  1.198174362  3.366764594  0.436731350  1.496298252      C   \n",
      "4    0030d7ee-573d-405e-9609-2c12a364a307   1.342353691   0.186825242  1.417049448  2.794842394  4.601687174  0.000401808  0.728629210      A   \n",
      "..                                    ...           ...           ...          ...          ...          ...          ...          ...    ...   \n",
      "195  09be4458-8014-43c4-8949-8c0ce6ed73bc  -0.154878810   0.540906430  1.788264085  1.508818816  0.910135044  0.834169129  0.688177553      A   \n",
      "196  09c77320-7589-4e1e-a6a5-f511f3120243   0.129492836  -0.341949259  0.329053151  2.119824151  2.663167629  0.165227703  0.443896948      C   \n",
      "197  09ec5ad5-3fb2-4af2-ba8f-2d5caf596f29  -0.328287701   0.999725479  0.805876803  0.546396220  0.898798931  3.262588560  1.908175969      D   \n",
      "198  09f93228-6097-4ac6-9ca2-5f0d56bb0f07  -1.759683195   0.691659807  0.189194235  4.748501666  2.536567240  0.358225617  0.294294717      A   \n",
      "199  0a08dc0e-118d-4ca7-84e2-ce91a8d95b16  -0.904449691  -2.426433209  1.270394625  4.393587214  4.735570408  0.221149455  0.101985907      C   \n",
      "\n",
      "    treatment t_ob             y          y_ob numerator_pre     numerator denominator_pre denominator       weight        day_        effect  \n",
      "0           0    1  -0.276689490   1.033698576   0.147830290  -0.276689490              11          12  0.245386871  2023-11-03  11.207578360  \n",
      "1           1    0  25.199689830   0.501502249   6.753580512  25.199689830               7           8  0.623936088  2023-11-03  14.796106486  \n",
      "2           0    1  -3.840697601   2.084558089  -2.721380925  -3.840697601               5           5  0.113941160  2023-11-03   9.670476049  \n",
      "3           0    0   3.687920839   0.728093352   2.860594060   3.687920839               6           4  0.621547682  2023-11-03   8.090702117  \n",
      "4           1    1  18.482986178   0.892991166   6.522697065  18.482986178              13          19  0.372828283  2023-11-03  16.678358000  \n",
      "..        ...  ...           ...           ...           ...           ...             ...         ...          ...         ...           ...  \n",
      "195         1    1  13.014687451   0.206201756   6.488997478  13.014687451               3           4  0.160803281  2023-11-03  15.626185653  \n",
      "196         1    1   8.557721926   2.094400005   1.456781596   8.557721926               6           8  0.337158554  2023-11-03   9.848489230  \n",
      "197         1    0  16.821854143  -0.561610203   5.378841512  16.821854143               7           7  0.936844045  2023-11-03  13.301958938  \n",
      "198         0    1   0.752843145   0.256560343  -0.565400955   0.752843145               3           0  0.181820555  2023-11-03   7.949346093  \n",
      "199         0    0  -2.828411667   1.480747708  -0.997516082  -2.828411667               8           5  0.701510144  2023-11-03   6.149275881  \n",
      "\n",
      "[200 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "import fast_causal_inference.dataframe.regression as Regression\n",
    "model = Regression.Wls(weight='1', use_bias=True)\n",
    "model.fit('y~x1+x2+x3', df)\n",
    "model.summary()\n",
    "effect_df = model.effect('x1+x2+x3', df)\n",
    "effect_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137b4a9-aa68-492d-8d74-0e3fc3f23074",
   "metadata": {},
   "source": [
    "### logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9514809-d619-4d0d-83fb-4de98b75a258",
   "metadata": {},
   "source": [
    "In statistics, the [logistic model (or logit model)](https://en.wikipedia.org/w/index.php?title=Logistic_regression) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). To estimate logistic model, we use **gradient descent**, an optimization algorithm.\n",
    "\n",
    "$$p(x)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "762159d3-fae1-498e-9d21-ed481e40a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 log-likelihood:  -5687.905578239838\n",
      "iter 2 log-likelihood:  -5627.901418714685\n",
      "iter 3 log-likelihood:  -5626.94770216139\n",
      "iter 4 log-likelihood:  -5626.947352745007\n",
      "The results have converged and the calculation has been stopped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept</td>\n",
       "      <td>0.083472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x1</td>\n",
       "      <td>0.957999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x2</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x3</td>\n",
       "      <td>0.534323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x4</td>\n",
       "      <td>-0.006258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>x5</td>\n",
       "      <td>-0.020528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>x_long_tail1</td>\n",
       "      <td>-0.036267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>x_long_tail2</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              x      beta\n",
       "0     intercept  0.083472\n",
       "1            x1  0.957999\n",
       "2            x2  0.217600\n",
       "3            x3  0.534323\n",
       "4            x4 -0.006258\n",
       "5            x5 -0.020528\n",
       "6  x_long_tail1 -0.036267\n",
       "7  x_long_tail2  0.000232"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fast_causal_inference\n",
    "from fast_causal_inference.dataframe.regression import Logistic\n",
    "table = 'test_data_small'\n",
    "df = fast_causal_inference.readStarRocks(table)\n",
    "X = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_long_tail1', 'x_long_tail2']\n",
    "Y = 't_ob'\n",
    "logit = Logistic(tol=1e-6, iter=500)\n",
    "logit.fit(Y, X, df)\n",
    "logit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93584bd-5e34-4963-b390-2a01b0451bca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b50503-7ef0-4e32-8ed2-ed4415902199",
   "metadata": {},
   "source": [
    "\n",
    "[instrumental variables (IV)](https://en.wikipedia.org/wiki/Instrumental_variables_estimation) is a method used in statistics, econometrics, epidemiology, and related disciplines to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment. \n",
    "\n",
    "The idea behind IV is to use a variable, known as an instrument, that is correlated with the endogenous explanatory variables (the variables that are correlated with the error term), but uncorrelated with the error term itself. This allows us to isolate the variation in the explanatory variable that is purely due to the instrument and thus uncorrelated with the error term, which can then be used to estimate the causal effect of the explanatory variable on the dependent variable.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "1. $t_1 = treatment + X_1 + X_2$\n",
    "2. $Y = \\hat t_1 + X_1 + X_2$\n",
    "\n",
    "- $X_1$ and $X_2$ are independent variables or predictors.\n",
    "- $t_1$ is the dependent variable that you are trying to explain or predict.  \n",
    "- $treatment$ is an independent variable representing some intervention or condition that you believe affects $t_1$. \n",
    "- $Y$ is the dependent variable that you are trying to explain or predict\n",
    "-  $\\hat t_1$ is the predicted value of $t_1 from the first equation\n",
    "\n",
    "\n",
    "We first regress $X_3$ on the treatment and the other exogenous variables $X_1$ and $X_2$ to get the predicted values $\\hat t_1$. Then, we replace $t_1$ with $\\hat t_1$ in the second equation and estimate the parameters. This gives us the causal effect of $t_1$ on $Y$, purged of the endogeneity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce71fb7b-faa1-4f7e-a2ee-3403b1f47ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "  lm( formula = y ~ x1 + x2 + x3 )\n",
      "\n",
      "Coefficients:\n",
      ".               Estimate    Std. Error  t value     Pr(>|t|)    \n",
      "(Intercept)     -2742.218377 501.046885  -5.472978   0.000000    \n",
      "x1              4578.692890 7548.134776 0.606599    0.544131    \n",
      "x2              -838.434135 12546.344968 -0.066827   0.946721    \n",
      "x3              -180.747094 2302.693725 -0.078494   0.937437    \n",
      "\n",
      "Residual standard error: 9.251235 on 9996 degrees of freedom\n",
      "Multiple R-squared: 0.474922, Adjusted R-squared: 0.474765\n",
      "F-statistic: 3013.725638 on 3 and 9996 DF,  p-value: 0.000000\n",
      "\n",
      "\n",
      "Call:\n",
      "  lm( formula = y ~ x1 + x2 + x3 )\n",
      "\n",
      "Coefficients:\n",
      ".               Estimate    Std. Error  t value     Pr(>|t|)    \n",
      "(Intercept)     -2742.218377 501.046885  -5.472978   0.000000    \n",
      "x1              4578.692889 7548.134776 0.606599    0.544131    \n",
      "x2              -838.434135 12546.344968 -0.066827   0.946721    \n",
      "x3              -180.747094 2302.693725 -0.078494   0.937437    \n",
      "\n",
      "Residual standard error: 9.251235 on 9996 degrees of freedom\n",
      "Multiple R-squared: 0.474922, Adjusted R-squared: 0.474765\n",
      "F-statistic: 3013.725638 on 3 and 9996 DF,  p-value: 0.000000\n",
      "\n",
      "\n",
      "Call:\n",
      "  lm( formula = y ~ x1 + x2 + x3 )\n",
      "\n",
      "Coefficients:\n",
      ".               Estimate    Std. Error  t value     Pr(>|t|)    \n",
      "(Intercept)     -2742.218377 501.046884  -5.472978   0.000000    \n",
      "x1              4578.692890 7548.134775 0.606599    0.544131    \n",
      "x2              -838.434135 12546.344967 -0.066827   0.946721    \n",
      "x3              -180.747094 2302.693725 -0.078494   0.937437    \n",
      "\n",
      "Residual standard error: 9.251235 on 9996 degrees of freedom\n",
      "Multiple R-squared: 0.474922, Adjusted R-squared: 0.474765\n",
      "F-statistic: 3013.725638 on 3 and 9996 DF,  p-value: 0.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fast_causal_inference.dataframe.regression as Regression\n",
    "model = Regression.IV()\n",
    "model.fit(df,formula='y~(t_ob~treatment)+x1+x2')\n",
    "model.summary()\n",
    "\n",
    "df.iv_regression('y~(t_ob~treatment)+x1+x2').show()\n",
    "df.agg(Regression.iv_regression('y~(t_ob~treatment)+x1+x2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87998b4c-30d2-427c-bcfb-7402acc8d0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
