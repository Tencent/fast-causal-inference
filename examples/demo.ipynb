{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c049a0-885f-41f3-9cba-a42917635da0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Fast Causal inference\n",
    "\n",
    "Fast Causal Inference is Tencent's first open-source causal inference project. It is an OLAP-based high-performance causal inference (statistical model) computing library, which solves the performance bottleneck of existing statistical model libraries (R/Python) under big data, and provides causal inference capabilities for massive data execution in seconds and sub-seconds. At the same time, the threshold for using statistical models is lowered through the SQL language, making it easy to use in production environments. At present, it has supported the causal analysis of WeChat-Search, WeChat-Video-Account and other businesses, greatly improving the work efficiency of data scientists.\n",
    "![Example Image](https://github.com/Tencent/fast-causal-inference/raw/main/docs/images/fast-causal-inference3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a303640-afd2-46c4-ad50-9d9036d6c57e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb08a2-9751-43d0-84cb-ff82e87b0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279fa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fast_causal_inference\n",
    "ais = fast_causal_inference.FCIProvider('all_in_sql')\n",
    "df = ais.readClickHouse('test_data_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c829be-2ee3-4d0a-a0cc-e20c0595109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f77e99-6844-42df-ab0c-c89fa83d6e0c",
   "metadata": {},
   "source": [
    "# know your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2dc3b-7ed2-431d-9878-22968ad72149",
   "metadata": {},
   "source": [
    "### test data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad3a91-3874-4025-a1d0-8ca45586f6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n = 10000\n",
    "\n",
    "################################## generate covariables #############################################\n",
    "# Generate x1-x5, they come from different distributions and have different variances\n",
    "x1 = np.random.normal(0, 1, n) # Normal distribution, variance is 1\n",
    "x2 = np.random.normal(0, 2, n)  # Normal distribution, variance is 4\n",
    "x3 = np.random.exponential(1, n)  # Exponential distribution, variance is 1\n",
    "x4 = np.random.exponential(2, n)  # Exponential distribution, variance is 4\n",
    "x5 = np.random.uniform(0, 5, n)  # Uniform distribution, variance is approximately 1.33\n",
    "weight = np.random.uniform(0, 1, n) # use for sample reweighing\n",
    "\n",
    "# Generate x6-x7, they are long-tail distributed data\n",
    "x_long_tail1 = np.random.pareto(3, n)  # Pareto distribution\n",
    "x_long_tail2 = np.random.pareto(2, n)  # Pareto distribution\n",
    "\n",
    "# Generate x_cat1, it is a discrete variable of string type\n",
    "x_cat1 = np.random.choice(['A', 'B', 'C', 'D', 'E'], n)\n",
    "###################################################################################################### \n",
    "\n",
    "\n",
    "\n",
    "################################## generate exprimental data ######################################### \n",
    "# Generate treatment, it is a binary random variable\n",
    "treatment = np.random.choice([0, 1], n)\n",
    "\n",
    "\n",
    "# Generate y \n",
    "# y is highly correlated with x1, x2, x3,x_long_tail2 and treatment\n",
    "# and there is heterogeneity on x_cat1 and x1,x2,x4\n",
    "y_pre = x1 + 2*x2 + 3*x3 + 3*np.log(x_long_tail2+1) + np.random.normal(0, 1, n)\n",
    "y = y_pre + 5*treatment + treatment*(x1+x2**2+np.log(x4+1)+(x_long_tail1>1))*2 + np.random.normal(0, 2, n)\n",
    "y[x_cat1 == 'A'] += 3\n",
    "y[x_cat1 == 'B'] -= 2\n",
    "\n",
    "# Generate ratio metric: numerator/denominator, for example click/show\n",
    "numerator_pre = y_pre \n",
    "numerator = y \n",
    "denominator_pre = x1 + 2*x5 + 3*np.log(x_long_tail1+1) + np.random.normal(0, 1, n)\n",
    "denominator = denominator_pre + 2*treatment + treatment*(x1)*2 + np.random.normal(0, 2, n)\n",
    "###################################################################################################### \n",
    "\n",
    "\n",
    "################################## generate observational data ####################################### \n",
    "# use x1,x2,x3 before\n",
    "# Generate linear combination for t_ob\n",
    "linear_combination_t = 1*x1 + 0.2*x2 + 0.5 * x3 + 0.1 * np.random.normal(0, 1, n)\n",
    "\n",
    "# Convert the linear combination into a probability using the logistic function\n",
    "prob_t = 1 / (1 + np.exp(-linear_combination_t))\n",
    "\n",
    "# Generate binary variable t based on the probability\n",
    "t_ob = np.random.binomial(1, prob_t)\n",
    "\n",
    "# Generate linear combination for y\n",
    "linear_combination_y = 0.5 * x1 - 0.25 * x2 + 0.1 * x3 + 0.5 * t_ob + 0.1 * np.random.normal(0, 1, n)\n",
    "\n",
    "# Generate target variable y\n",
    "y_ob = linear_combination_y + np.random.normal(0, 1, n)\n",
    "###################################################################################################### \n",
    "\n",
    "\n",
    "\n",
    "# get DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x1': x1,\n",
    "    'x2': x2,\n",
    "    'x3': x3,\n",
    "    'x4': x4,\n",
    "    'x5': x5,\n",
    "    'x_long_tail1': x_long_tail1,\n",
    "    'x_long_tail2': x_long_tail2,\n",
    "    'x_cat1': x_cat1,\n",
    "    'treatment': treatment,\n",
    "    't_ob':t_ob,\n",
    "    'y': y,\n",
    "    'y_ob': y_ob,\n",
    "    'numerator_pre':numerator_pre,\n",
    "    'numerator':numerator,\n",
    "    'denominator_pre':denominator_pre,\n",
    "    'denominator':denominator,\n",
    "    'weight' : weight\n",
    "})\n",
    "\n",
    "\n",
    "# show data \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f3e29-ad4e-4318-a054-cb1c8fc3e716",
   "metadata": {},
   "source": [
    "Covariables\n",
    "\n",
    "- `x1`: This variable follows a normal distribution with a mean of 0 and a variance of 1.\n",
    "- `x2`: This variable also follows a normal distribution, but with a mean of 0 and a larger variance of 4.\n",
    "- `x3`: This variable is generated from an exponential distribution with a rate parameter of 1, resulting in a variance of 1.\n",
    "- `x4`: Similar to `x3`, this variable is generated from an exponential distribution, but with a larger rate parameter of 2, resulting in a variance of 4.\n",
    "- `x5`: This variable is generated from a uniform distribution between 0 and 5, with an approximate variance of 1.33.\n",
    "- `weight`: This variable is generated from a uniform distribution between 0 and 1 and can be used for sample reweighing.\n",
    "- `x_long_tail1`: This variable follows a Pareto distribution with a shape parameter of 3, representing a long-tailed distribution.\n",
    "- `x_long_tail2`: Similarly, this variable follows a Pareto distribution with a shape parameter of 2.\n",
    "- `x_cat1`: This variable is a discrete variable of string type, randomly chosen from the categories 'A', 'B', 'C', 'D', and 'E'.\n",
    "\n",
    "Experimental data\n",
    "- `treatment`: This variable represents the treatment assignment and is a binary random variable.\n",
    "\n",
    "- `y`: The target variable `y` is highly correlated with `x1`, `x2`, `x3`, `x_long_tail2`, and `treatment`. There is also heterogeneity in the relationship with `x_cat1` and `x1`, `x2`, `x4`. It is generated by adding `y_pre` with treatment effects, interaction terms, and random noise.\n",
    "\n",
    "- `numerator_pre`: This variable is a precursor to the numerator of a ratio metric and is highly correlated with `y_pre`.\n",
    "\n",
    "- `numerator`: The numerator of the ratio metric is derived from `numerator_pre` and is highly correlated with `y`.\n",
    "\n",
    "- `denominator_pre`: This variable is a precursor to the denominator of a ratio metric and is generated based on `x1`, `x5`, and `x_long_tail1`.\n",
    "\n",
    "- `denominator`: The denominator of the ratio metric is derived from `denominator_pre` and is influenced by treatment effects, interaction terms with `x1`, and random noise.\n",
    "\n",
    "Observational data \n",
    "- `t_ob`: This binary variable is generated based on a linear combination of `x1`, `x2`, and `x3`. The linear combination is converted into a probability using the logistic function, and `t_ob` is generated by sampling from a binomial distribution with the probability.\n",
    "\n",
    "- `y_ob`: The target variable `y_ob` is generated based on a linear combination of `x1`, `x2`, `x3`, and `t_ob`, along with random noise. It represents the outcome variable in the observational data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5bda7-d6b8-44cb-a3e8-27cadcc3c43c",
   "metadata": {},
   "source": [
    "### data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e8a39-69d2-4a5c-8c6f-b66e075d5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ais.readClickHouse('test_data_small')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0a121-a978-45b8-bd0a-dca42ee1cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66739f01-aea3-4a63-9778-f8017c4785f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data descirption\n",
    "df.describe('*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aecdae-ef4d-4842-bb7f-45a0a320ff48",
   "metadata": {},
   "source": [
    "#### histplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3a444-cf80-40eb-a671-e29681fb89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histplot\n",
    "\n",
    "from fast_causal_inference.lib.tools import *\n",
    "col = 'x1'\n",
    "histplot('test_data_small',col,bin_num=50)\n",
    "\n",
    "col = 'x_cat1'\n",
    "histplot('test_data_small',col,bin_num=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5aae90-e327-44d4-ad2b-8873dbaf45c9",
   "metadata": {},
   "source": [
    "#### boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f495c-10a2-46e8-91e6-ee06647c5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "\n",
    "from fast_causal_inference.lib.tools import *\n",
    "\n",
    "col = 'x_long_tail1'\n",
    "boxplot('test_data_small',col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01fd229-ffed-4b93-a6a9-0c17ac54835e",
   "metadata": {},
   "source": [
    "# AB experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058363b-5e71-48dc-ab22-cd51d85e8c8c",
   "metadata": {},
   "source": [
    "### ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af779c9a-822b-44f2-be2f-bf242eb5e792",
   "metadata": {},
   "source": [
    "- In A/B test analysis, **t-test** is widely used to test whether the average of treatment variant is statistically significantly different from the average of the control variant. However, the mean and variance formula only applied to i.i.d (independent and identically distributed) random variables, and in real business cases our metrics are more complex. Mostly, business metrics are defined as ratios, for example, Clickthrough rate or CTR which is defined as Clicks/Views. Here, we will *utilize the delta method to approximate the variance of the metrics ratio*.\n",
    "$$t=\\frac{\\bar X_B-\\bar X_A}{\\sqrt{Var(\\bar X_A)+Var(\\bar X_B)}}$$\n",
    "\n",
    "$$Var(\\bar X)=\\frac{1}{n}\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2 $$\n",
    "- **Delta method** extends the normal approximations of the central limit theorem. Delta method approximates asymptotically normal random variables by applying the Taylor series on the function of random variables. We can estimate the variance of x/y as follows:\n",
    "$$v=(1/y,-x/y^2)|_{x=\\bar x, y=\\bar y )}$$\n",
    "\n",
    "$$\\mathop{{M}}\\nolimits_{{2 \\times 2}}=\\frac{1}{n}{ \\left[ {\\begin{array}{*{20}{c}}\n",
    "{var(x)}&{cov(x,y)}\\\\\n",
    "{cov(x,y)}&{var(y)}\\\\\n",
    "\\end{array}} \\right] }={ \\left[ {\\mathop{{m}}\\nolimits_{{ij}}} \\right] }  $$\n",
    "\n",
    "$$var(x/y)=v*\\mathop{{M}}\\nolimits_{{2 \\times 2}}*v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99a160-627e-4f2e-959d-dc1d5f2931c8",
   "metadata": {},
   "source": [
    "- <font size=\"4\">case1: use deltamethod to do t-test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d066d0-9dd5-4bc3-a62d-21d504e941e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ttest_2samp('avg(numerator)/avg(denominator)','treatment','two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e1391-90e8-4272-9fd7-efb0e10e68e8",
   "metadata": {},
   "source": [
    "- <font size=\"4\">case2: use deltamethod to do t-test and **CUPED** to reduce variance</font>  \n",
    "\n",
    "\n",
    "    - We offer a feature in ttest_2samp that utilizes [CUPED (Controlled-experiment Using Pre-Experiment Data)](https://ai.stanford.edu/~ronnyk/2013-02CUPEDImprovingSensitivityOfControlledExperiments.pdf) This technique adjusts metrics using pre-experiment data from both control and treatment groups, aiming to decrease metric variability.\n",
    "    - To further reduce variance, you can combine multiple metrics using the '+' operator.\n",
    "    - Additionally, ttest_2samp allows you to perform a t-test grouped by any dimensions of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c6c8c-4d0c-4ee7-8fa3-88e51272b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ttest_2samp('avg(numerator)/avg(denominator)','treatment','two-sided',X='avg(numerator_pre)/avg(denominator_pre)+avg(x1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6c590-4286-43ce-ae35-d46ba1945498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('x_cat1').ttest_2samp('avg(numerator)/avg(denominator)','treatment','two-sided',X='avg(numerator_pre)/avg(denominator_pre)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372f881-39e5-4b31-a133-a4699f7081e6",
   "metadata": {},
   "source": [
    "- <font size=\"4\">case3: ttest_1samp </font>  \n",
    "    - ttest_1samp can be used in pair test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca383d6-476a-4058-ae3e-1e18b599374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ttest_1samp('avg(numerator)/avg(denominator)','two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c219-bb55-42c5-95c3-8ad0e2477236",
   "metadata": {},
   "source": [
    "### MWU test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc609f8a-24e5-4106-a5cd-81cd6440467e",
   "metadata": {},
   "source": [
    "The [Mann–Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2560909-59f1-4244-a282-da8703b86bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mann_whitney_utest('numerator', 'treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab336479-8dca-4b22-b1fa-66a4d3c15130",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf25177-a5ed-4f54-8d0e-2ad1be3b02a3",
   "metadata": {},
   "source": [
    "**SRM (Sample ratio mismatch)** is an experimental flaw where the expected traffic allocation doesn’t fit with the observed visitor number for each testing variation. We do this using the chi-squared test of independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bef2a-d380-4a5f-b7fc-c4523224b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.srm('numerator', 'treatment', [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a57cf-c74d-48b2-9390-ffbeaaa634e4",
   "metadata": {},
   "source": [
    "# Regression-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373bd1f-1926-439a-b57b-ff9ccea2aaab",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36b6e3-fd2d-47b3-89cb-5b46c18dfac9",
   "metadata": {},
   "source": [
    "In statistics, [linear regression](https://en.wikipedia.org/wiki/Linear_regression) is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression. Written in matrix notation as:\n",
    "$$y=X\\beta+\\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f603ba67-8f00-43a7-8296-c058ab8405eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ceb56-9228-4044-8e24-6cb02387951b",
   "metadata": {},
   "source": [
    "In statistics, [ordinary least squares (OLS)](https://en.wikipedia.org/wiki/Ordinary_least_squares) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\n",
    "$$S(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left| y_i - \\sum_{j=1}^p X_{ij}\\beta_j\\right|^2 = \\left\\|\\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\right\\|^2.$$\n",
    "We can leverage **matrix multiplication** to compute Ordinary Least Squares (OLS), which is highly efficient in the OLAP engine we are using.\n",
    "$$\\hat{\\boldsymbol{\\beta}} = \\left( \\mathbf{X}^{\\operatorname{T}} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\operatorname{T}} \\mathbf y.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f50a78-6298-4bc3-8732-562392a402f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols\n",
    "df.ols('y~x1+x2+x3+weight', use_bias=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d8412-d83d-457f-99e5-c3d693da4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fast_causal_inference.dataframe.regression as Regression\n",
    "table = 'test_data_small'\n",
    "df = fast_causal_inference.readClickHouse(table)\n",
    "model = Regression.Ols(True)\n",
    "model.fit('y~x1+x2+t_ob', df)\n",
    "model.summary()\n",
    "effect_df = model.effect('x1+x2+x3', df)\n",
    "effect_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4d3ac-bc9b-4f2f-b610-2a96f46fbaa6",
   "metadata": {},
   "source": [
    "#### WLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52063db6-fce9-4794-bfd0-2b7a4986ea45",
   "metadata": {},
   "source": [
    "[Weighted least squares (WLS)](https://en.wikipedia.org/wiki/Weighted_least_squares), also known as weighted linear regression,[1][2] is a generalization of ordinary least squares and linear regression in which knowledge of the unequal variance of observations (heteroscedasticity) is incorporated into the regression. WLS is also a specialization of generalized least squares, when all the off-diagonal entries of the covariance matrix of the errors, are null.\n",
    "$$  \\underset{\\boldsymbol\\beta}{\\operatorname{arg\\ min}}\\, \\sum_{i=1}^{n} w_i \\left|y_i - \\sum_{j=1}^{m} X_{ij}\\beta_j\\right|^2 =\n",
    "  \\underset{\\boldsymbol\\beta}{\\operatorname{arg\\ min}}\\, \\left\\|W^\\frac{1}{2}\\left(\\mathbf{y} - X\\boldsymbol\\beta\\right)\\right\\|^2.$$\n",
    "We can leverage **matrix multiplication** to compute Weighted least squares (WLS), which is highly efficient in the OLAP engine we are using.\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (X^\\textsf{T} W X)^{-1} X^\\textsf{T} W \\mathbf{y}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97250e-f0fc-4eb2-ac56-b24e2baff801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fast_causal_inference.dataframe.regression as Regression\n",
    "model = Regression.Wls(weight='1', use_bias=True)\n",
    "model.fit('y~x1+x2+x3', df)\n",
    "model.summary()\n",
    "effect_df = model.effect('x1+x2+x3', df)\n",
    "effect_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971b225-82cc-453a-8e94-703177cb184c",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba654ab-cd84-44d8-a81d-797a3a5c4e05",
   "metadata": {},
   "source": [
    "**Lasso (Least Absolute Shrinkage and Selection Operator)** is a method used in regression analysis that performs both variable selection and regularization. This enhances the prediction accuracy and interpretability of the statistical model it produces. Lasso introduces a penalty term to the loss function of the least square method, which is the absolute value of the magnitude of the coefficients. This results in some coefficients being shrunk to zero, effectively selecting a simpler model that does not include those coefficients. To estimate Lasso, we use **gradient descent**, an optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e95e5-23e3-460c-8dbb-dcb6ac7393b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fast_causal_inference.dataframe.regression as Regression\n",
    "df.stochastic_logistic_regression('y~x1+x2+x3', learning_rate=0.00001, l1=0.1, batch_size=15, method='Lasso').show()\n",
    "df.agg(Regression.stochastic_logistic_regression('y~x1+x2+x3', learning_rate=0.00001, l1=0.1, batch_size=15, method='SGD')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137b4a9-aa68-492d-8d74-0e3fc3f23074",
   "metadata": {},
   "source": [
    "### logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9514809-d619-4d0d-83fb-4de98b75a258",
   "metadata": {},
   "source": [
    "In statistics, the [logistic model (or logit model)](https://en.wikipedia.org/w/index.php?title=Logistic_regression) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). To estimate logistic model, we use **gradient descent**, an optimization algorithm.\n",
    "\n",
    "$$p(x)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762159d3-fae1-498e-9d21-ed481e40a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fast_causal_inference\n",
    "from fast_causal_inference.dataframe.regression import Logistic\n",
    "table = 'test_data_small'\n",
    "df = fast_causal_inference.readClickHouse(table)\n",
    "X = ['x1', 'x2', 'x3', 'x4', 'x5', 'x_long_tail1', 'x_long_tail2']\n",
    "Y = 't_ob'\n",
    "logit = Logistic(tol=1e-6, iter=500)\n",
    "logit.fit(Y, X, df)\n",
    "logit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93584bd-5e34-4963-b390-2a01b0451bca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b50503-7ef0-4e32-8ed2-ed4415902199",
   "metadata": {},
   "source": [
    "\n",
    "[instrumental variables (IV)](https://en.wikipedia.org/wiki/Instrumental_variables_estimation) is a method used in statistics, econometrics, epidemiology, and related disciplines to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment. \n",
    "\n",
    "The idea behind IV is to use a variable, known as an instrument, that is correlated with the endogenous explanatory variables (the variables that are correlated with the error term), but uncorrelated with the error term itself. This allows us to isolate the variation in the explanatory variable that is purely due to the instrument and thus uncorrelated with the error term, which can then be used to estimate the causal effect of the explanatory variable on the dependent variable.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "1. $t_1 = treatment + X_1 + X_2$\n",
    "2. $Y = \\hat t_1 + X_1 + X_2$\n",
    "\n",
    "- $X_1$ and $X_2$ are independent variables or predictors.\n",
    "- $t_1$ is the dependent variable that you are trying to explain or predict.  \n",
    "- $treatment$ is an independent variable representing some intervention or condition that you believe affects $t_1$. \n",
    "- $Y$ is the dependent variable that you are trying to explain or predict\n",
    "-  $\\hat t_1$ is the predicted value of $t_1 from the first equation\n",
    "\n",
    "\n",
    "We first regress $X_3$ on the treatment and the other exogenous variables $X_1$ and $X_2$ to get the predicted values $\\hat t_1$. Then, we replace $t_1$ with $\\hat t_1$ in the second equation and estimate the parameters. This gives us the causal effect of $t_1$ on $Y$, purged of the endogeneity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71fb7b-faa1-4f7e-a2ee-3403b1f47ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fast_causal_inference.dataframe.regression as Regression\n",
    "model = Regression.IV()\n",
    "model.fit(df,formula='y~(t_ob~treatment)+x1+x2')\n",
    "model.summary()\n",
    "\n",
    "df.iv_regression('y~(t_ob~treatment)+x1+x2').show()\n",
    "df.agg(Regression.iv_regression('y~(t_ob~treatment)+x1+x2')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a73f9c-2970-4344-955b-45c2579e89d4",
   "metadata": {},
   "source": [
    "# Obeservational Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcb96a-a895-4d04-ac0b-4713ed0f1d6d",
   "metadata": {},
   "source": [
    "### Case1- PSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33358d29-b7ef-4f2c-85d0-1ece8c7d2c1b",
   "metadata": {},
   "source": [
    "[Propensity score matching (PSM)](https://en.wikipedia.org/wiki/Propensity_score_matching) is a quasi-experimental method in which the researcher uses statistical techniques to construct an artificial control group by matching each treated unit with a non-treated unit of similar characteristics. Using these matches, the researcher can estimate the impact of an intervention.\n",
    "![Example Image](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_propensity-score-matching.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bc2a6-48ee-431a-b02f-823200ae513b",
   "metadata": {},
   "source": [
    "#### predict propensity score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db208aab-ca3e-4c2b-91c5-b3e99714f66c",
   "metadata": {},
   "source": [
    "Once we have collected the data, we can build the propensity model predicting the probability of receiving the treatment given the confounders. Typically, logistic regression is used for this classification model. Let’s build a propensity model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5dec7-dcfa-4c56-8e85-104fba40b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fast_causal_inference\n",
    "Y='y'\n",
    "T='t_ob'\n",
    "table = 'test_data_small'\n",
    "X = [ 'x1', 'x2', 'x3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f687c2-bcef-487c-bc7a-e30324969238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fast_causal_inference.dataframe.regression as Regression\n",
    "model = Regression.StochasticLogisticRegression(learning_rate=0.00001, l1=0.1, batch_size=15, method='SGD')\n",
    "\n",
    "df = ais.readClickHouse('test_data_small')\n",
    "model.fit('y~x1+x2+x3', df)\n",
    "effect_df = model.effect('x1+x2+x3', df)\n",
    "effect_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9ae99-471e-496d-b3aa-ba26e1a2ad0c",
   "metadata": {},
   "source": [
    "#### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d98b28-cc1d-47ee-ad4a-2f94ff9ff7d1",
   "metadata": {},
   "source": [
    "We will be performing one-to-one matching to find the most similar control records for each passenger in the treatment group. Matching based on the propensity score, which is a balancing score, allows us to ensure that the distribution of confounders between the matched records is likely to be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0270ae99-6d7e-4a90-b001-db16b3f4b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "table_match = f\"{table}_{int(time.time())}_matched\"\n",
    "fast_causal_inference.clickhouse_create_view(clickhouse_view_name=table_match, sql_statement=f\"\"\"\n",
    "select *,caliperMatching(if({T}=1,1,-1),effect,0.05) AS matchingIndex \n",
    "from {effect_df.getTableName()} where matchingIndex!=0 \"\"\", primary_column=\"matchingIndex\",is_force_materialize=True, is_sql_complete=True, is_use_local=True)\n",
    "match_df = ais.readClickHouse(table_match)\n",
    "match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961fd1c-2576-4b3b-b2ad-19cb56b016cc",
   "metadata": {},
   "source": [
    "#### Balance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a599413-140e-499b-b284-0f031d398397",
   "metadata": {},
   "source": [
    "It’s time to evaluate how good the matching was. Let’s inspect if the groups look more comparable in terms of the confounders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e23384-0e7f-469d-a31a-8a1c28cb3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before matched\n",
    "from fast_causal_inference.lib.tools import *\n",
    "SMD(effect_df.getTableName(),T,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad51e0c-2895-41fd-9819-e81d027c0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_causal_inference.lib.tools import *\n",
    "matching_plot(effect_df.getTableName(),T,'effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ab6c3-763f-4408-925d-7e7ef7f8e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After matched\n",
    "from fast_causal_inference.lib.tools import *\n",
    "SMD(match_df.getTableName(),T,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6afb4-8fd1-4178-a92a-be5db96a9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_causal_inference.lib.tools import *\n",
    "matching_plot(match_df.getTableName(),T,'effect')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11fa07-a840-456a-ad40-980718b8028f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluate treatment effect on the outcome\n",
    "\n",
    "Now, it’s time to familiarize ourselves with a few terms related to the treatment effect, also known as the causal effect. Looking at a small example with a continuous outcome may be the easiest way to get familiarized with it.\n",
    "##### DIM estimator\n",
    "After the matching process, we can consider the experimental and control groups as homogeneous, free from bias. This allows us to directly estimate the Average Treatment Effect (ATE) and compute its variance using the bootstrap method. The `ATEestimator` function also returns the confidence interval for the DIM(Difference in Means) estimator. As per the formula below:\n",
    "\n",
    "- $\\hat \\tau_{ipw}$ represents the ATE estimator. \n",
    "\n",
    "For each sample $i=1,...,N$:\n",
    "\n",
    "- $Y_i$ is the outcome variable\n",
    "- $Z_i$ represents the treatment\n",
    "- $X_i$ is the covariate.\n",
    "\n",
    "$$\\hat \\tau_{ATE}=\\frac{\\sum_{i=1}^N Y_iZ_i}{\\sum_{i=1}^NZ_i}-\\frac{\\sum_{i=1}^N Y_i(1-Z_i))}{\\sum_{i=1}^N(1-Z_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739cb56-8ae0-4af4-9817-a1ca5aa92d5e",
   "metadata": {},
   "source": [
    "##### IPW estimator\n",
    "Without resorting to matching, we can directly estimate the Average Treatment Effect (ATE) using all available samples. This process involves the use of the Inverse Probability Weighting (IPW) estimator for ATE estimation, with the variance calculated via the bootstrap method. The `IPWestimator` function also yields the confidence interval of the ATE. As illustrated in the formula below:\n",
    "\n",
    "- $\\hat \\tau_{ipw}$ is the IPW estimator. \n",
    "\n",
    "For each sample $i=1,...,N$:\n",
    "\n",
    "- $Y_i$ is the outcome variable\n",
    "- $Z_i$ is the treatment\n",
    "- $X_i$ is the covariate\n",
    "- $e(X_i)$ is the predicted propensity score.\n",
    "\n",
    "$$\\hat \\tau_{ipw}=\\frac{\\sum_{i=1}^N Y_iZ_i/e(X_i)}{\\sum_{i=1}^NZ_i/e(X_i)}-\\frac{\\sum_{i=1}^N Y_i(1-Z_i)/(1-e(X_i))}{\\sum_{i=1}^N(1-Z_i)/(1-e(X_i))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820dbcf-b3b8-4f14-8c82-c5dcdfd74e79",
   "metadata": {},
   "source": [
    "### Case2- Uplift modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949cfc2c-c397-43d3-a3f4-4ac4449108e6",
   "metadata": {},
   "source": [
    "Uplift modeling is a set of causal inference techniques that leverage machine learning models to estimate the causal impact of a treatment on an individual's behavior.\n",
    "\n",
    "In this context:\n",
    "\n",
    "- \"Persuadables\" are individuals who respond positively to the treatment.\n",
    "- \"Sleeping dogs\" are individuals who exhibit a strong negative response to the treatment.\n",
    "- \"Lost causes\" are individuals who do not reach the desired outcome even with the treatment.\n",
    "- \"Sure things\" are individuals who always achieve the desired outcome, regardless of the treatment.\n",
    "\n",
    "The objective is to identify the \"persuadables\" to focus efforts on, avoid wasting resources on \"sure things\" and \"lost causes,\" and prevent unnecessary intervention for \"sleeping dogs.\"\n",
    "\n",
    "![Example Image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSsUVyoXY2SaSAdDVAIur1ssPPU8NZ0Qwlff252bgd8zVDOye08yErMf-aIZy_hNOMk_RY&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a591b75-ea7a-4f70-8540-38c3f464c8ef",
   "metadata": {},
   "source": [
    "#### data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c132d38-4cfe-4ebd-bc6b-f324eb60734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'test_data_small'\n",
    "Y='y'\n",
    "T='treatment'\n",
    "X = 'x1+x2+x3+x4+x5+x_long_tail1+x_long_tail2'\n",
    "needcut_X = 'x1+x2+x3+x4+x5+x_long_tail1+x_long_tail2'\n",
    "\n",
    "from fast_causal_inference.lib.tools import *\n",
    "table_train,table_test = data_split(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c624f0-b4e4-4ad2-a801-cb27d9d5e9cd",
   "metadata": {},
   "source": [
    "#### causal tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8eb0e3-588c-4cd1-af36-18f65c5dcd17",
   "metadata": {},
   "source": [
    "The causal tree is a method used in [Recursive Partitioning for Heterogeneous Causal Effects](https://www.pnas.org/doi/10.1073/pnas.1510489113) to estimate the individual treatment effects within a population. It is a variant of traditional decision trees that aims to identify subgroups of individuals who respond differently to a treatment.\n",
    "\n",
    "The causal tree algorithm recursively partitions the data based on a set of covariates and their treatment assignment. It seeks to find the optimal splits that maximize the heterogeneity in treatment effects across the resulting subgroups. This allows for the identification of subpopulations that exhibit varying responses to the treatment.\n",
    "\n",
    "The causal tree approach is valuable in understanding and predicting individual treatment effects in situations where the treatment effect may vary across different subpopulations. It provides a useful tool for personalized decision-making and targeted interventions based on the identified subgroups with distinct treatment responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cbb29-cd04-42a8-a914-4fb4da002c2a",
   "metadata": {},
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f93632-bf57-48bd-8695-df6a920ae34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_causal_inference.lib.causaltree import CausalTree\n",
    "hte = CausalTree(depth = 3,min_sample_ratio_leaf=0.001)\n",
    "hte.fit(Y,T,X,needcut_X,table_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66061a2-59e9-41fb-857a-17a334170a45",
   "metadata": {},
   "source": [
    "##### tree visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218a83a-88d7-4cb3-847c-b921c644e60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "treeplot = hte.treeplot()\n",
    "treeplot.render('digraph.gv', view=False) # save to file digraph.gv.pdf\n",
    "treeplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea48ad7-b00d-4b2c-9c71-5d9aeb9edc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uplift curve in training data\n",
    "hte.hte_plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f337e87-79ed-42a2-a087-e5ee6ac24afa",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7ccca-2dd5-4aaf-b0bf-7a17b57519c6",
   "metadata": {},
   "source": [
    "Since actual uplift can't be observed for each individual, measure the uplift over a group of customers.\n",
    "\n",
    "Uplift Curve: plots the real cumulative uplift across the population\n",
    "- First, rank the test dataframe order by the predict uplift.\n",
    "- Next, calculate the cumulative percentage of visits in each group (treatment or control).\n",
    "- Finally, calculate the group's uplift at each percentage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bedad2-5486-43e4-b176-8e219674e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fast_causal_inference.lib.causaltree import CausalTree\n",
    "from fast_causal_inference.lib.metrics import *\n",
    "\n",
    "table_test_predict = f'{table_test}_pred'\n",
    "# clickhouse_drop_view(clickhouse_view_name=table_test_predict) # drop table\n",
    "hte.effect_2_clickhouse(table_test_predict,\n",
    "                        table_input=table_test,\n",
    "                        keep_col='*')\n",
    "tmp1 = get_lift_gain(\"effect\", Y, T, table_test_predict,discrete_treatment=True, K=100)\n",
    "print(tmp1)\n",
    "hte_plot([tmp1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
